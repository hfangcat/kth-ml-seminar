<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on KTH Machine Learning Seminars</title>
    <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/</link>
    <description>Recent content in Posts on KTH Machine Learning Seminars</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© Copyright notice</copyright>
    <lastBuildDate>Thu, 08 Dec 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Wenbo Gong: Rhino: Deep Causal Temporal Relationship Learning with history-dependent noise</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-10/</link>
      <pubDate>Thu, 08 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-10/</guid>
      <description>Title: Rhino: Deep Causal Temporal Relationship Learning with history-dependent noise
Speaker: Wenbo Gong, Microsoft Research, Cambridge UK
Date and Time: Thursday, December 8, 3-4 pm (2-3 pm GMT)
Place: Zoom Meeting
Meeting ID: 691 6539 9062
Abstract:
Discovering causal relationships between different variables from time series data has been a long-standing challenge for many domains. Given the complexity of real-world relationships and the nature of observation in discrete time, the causal discovery method needs to consider non-linear relations between variables, instantaneous effects and history dependent noise.</description>
    </item>
    
    <item>
      <title>Agrin Hilmkil: Optimizing decisions with deep end-to-end causal inference</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-9/</link>
      <pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-9/</guid>
      <description>Title: Optimizing decisions with deep end-to-end causal inference
Speaker: Agrin Hilmkil, Microsoft Research, Cambridge UK
Date and Time: Thursday, November 10, 2-3 pm
Place: Fantum, Lindstedtsvägen 24, Floor 5
Video Link: Zoom Meeting
Meeting ID: 679 0294 8719
Abstract: In any type of decision making, the ability to predict the outcomes of different options will determine the quality of the decision made. However, traditional predictive models trained with supervised learning capture the biases of the underlying training data and are generally not robust to spurious correlations.</description>
    </item>
    
    <item>
      <title>Carl Doersch: Learning and transferring visual representations with few labels: BYOL &#43; CrossTransformers</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-8/</link>
      <pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-8/</guid>
      <description>Title: Learning and transferring visual representations with few labels: BYOL + CrossTransformers
Speaker: Carl Doersch, DeepMind
Date and Time: Tuesday, June 22, 1-2 pm
Place: Zoom Meeting
Meeting ID: 693 9289 5889 Pass code: 259475
Abstract: When encountering novelty, like new tasks and new domains, current visual representations struggle to transfer knowledge if trained on standard tasks like ImageNet classification. This talk explores how to build representations which better capture the visual world, and transfer better to new tasks.</description>
    </item>
    
    <item>
      <title>Mathilde Caron: Self-Supervised Learning of Visual Representations</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-5/</link>
      <pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-5/</guid>
      <description>Title: Self-Supervised Learning of Visual Representations
Speaker: Mathilde Caron, Facebook AI Research
Date and Time: Tuesday, May 18, 1-2 pm
Place: Zoom Meeting
Meeting ID: 628 7889 2739 Pass code: 053004
Abstract: Self-supervised learning is the problem of training deep neural network systems without using any manual annotations. Training deep networks typically requires large amounts of annotated data, which has limited their applications in fields where accessing annotations is difficult. Moreover, manual annotations are biased towards a specific task and towards the annotator&amp;rsquo;s own biases.</description>
    </item>
    
    <item>
      <title>Kai Han: Transformer in Transformer</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-2/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-2/</guid>
      <description>Title: Transformer in Transformer
Speaker: Kai Han, Huawei Noah’s Ark Lab
Date and Time: Tuesday, April 27, 1-2 pm
Place: Zoom Meeting
Meeting ID: 621 2899 7306 Pass code: 373072
Abstract: Transformer is a type of self-attention-based neural networks originally applied for NLP tasks. Recently, pure transformer-based models are proposed to solve computer vision problems. These visual transformers usually view an image as a sequence of patches while they ignore the intrinsic structure information inside each patch.</description>
    </item>
    
    <item>
      <title>Alaa El-Nouby: Training Vision Transformers for Image Retrieval</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-4/</link>
      <pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-4/</guid>
      <description>Title: Training Vision Transformers for Image Retrieval
Speaker: Alaa El-Nouby, Facebook AI Research and Inria Paris
Date and Time: Tuesday, April 20, 1-2 pm
Place: Zoom Meeting
Meeting ID: 699 6421 6598 Pass code: 600145
Abstract: Transformers have shown outstanding results for natural language understanding and, more recently, for image classification. We here extend this work and propose a transformer-based approach for image retrieval: we adopt vision transformers for generating image descriptors and train the resulting model with a metric learning objective, which combines a contrastive loss with a differential entropy regularizer.</description>
    </item>
    
    <item>
      <title>Hugo Touvron: Training data-efficient image transformers &amp; distillation through attention / Going deeper with Image Transformers</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-3/</link>
      <pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-3/</guid>
      <description>Title: Training data-efficient image transformers &amp;amp; distillation through attention / Going deeper with Image Transformers
Speaker: Hugo Touvron, Facebook AI Research and Sorbonne University
Date and Time: Tuesday, April 20, 1-2 pm
Place: Zoom Meeting
Meeting ID: 699 6421 6598 Pass code: 600145
Abstract:
Training data-efficient image transformers &amp;amp; distillation through attention
Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.</description>
    </item>
    
    <item>
      <title>Mostafa Dehghani: Scaling up Vision Models with Transformers</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-1/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-1/</guid>
      <description>Title: Scaling up Vision Models with Transformers
Speaker: Mostafa Dehghani, Google Brain
Date and Time: Tuesday, April 13, 1-2 pm
Place: Zoom Meeting
Meeting ID: 698 1002 6609 Pass code: 983918
Abstract: While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place.</description>
    </item>
    
    <item>
      <title>Alyosha Efros: The Revolution Will Not Be Supervised</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-6/</guid>
      <description>Title: The Revolution Will Not Be Supervised
Speaker: Alyosha Efros, University of California Berkeley
Date and Time: 2019
Place: Room 304, Teknikringen 14
Abstract: Computer vision has made impressive gains through the use of deep learning models trained with large-scale labeled data. However, labels require expertise and curation and are expensive to collect. Worse, semantic supervision often leads to models that can &amp;ldquo;cheat&amp;rdquo;. Can one discover useful visual representations without the use of explicitly curated labels?</description>
    </item>
    
    <item>
      <title>Daphna Weinshall</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-7/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-7/</guid>
      <description>Title:
Speaker: Daphna Weinshall, Hebrew University, Jerusalem
Date and Time: 2019
Place: Room 525, Teknikringen 14
Abstract:
Bio:
Organizer: Stefan Carlsson</description>
    </item>
    
  </channel>
</rss>
