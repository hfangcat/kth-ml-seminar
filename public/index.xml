<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>KTH Machine Learning Seminars</title>
    <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/</link>
    <description>Recent content on KTH Machine Learning Seminars</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© Copyright notice</copyright>
    <lastBuildDate>Tue, 22 Jun 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://www.csc.kth.se/cvap/cvg/ml-seminars/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Carl Doersch: Learning and transferring visual representations with few labels: BYOL &#43; CrossTransformers</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-8/</link>
      <pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-8/</guid>
      <description>Title: Learning and transferring visual representations with few labels: BYOL + CrossTransformers
Speaker: Carl Doersch, DeepMind
Date and Time: Tuesday, June 22, 1-2 pm
Place: Zoom Meeting
Meeting ID: 693 9289 5889 Pass code: 259475
Abstract: When encountering novelty, like new tasks and new domains, current visual representations struggle to transfer knowledge if trained on standard tasks like ImageNet classification. This talk explores how to build representations which better capture the visual world, and transfer better to new tasks.</description>
    </item>
    
    <item>
      <title>Mathilde Caron: Self-Supervised Learning of Visual Representations</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-5/</link>
      <pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-5/</guid>
      <description>Title: Self-Supervised Learning of Visual Representations
Speaker: Mathilde Caron, Facebook AI Research
Date and Time: Tuesday, May 18, 1-2 pm
Place: Zoom Meeting
Meeting ID: 628 7889 2739 Pass code: 053004
Abstract: Self-supervised learning is the problem of training deep neural network systems without using any manual annotations. Training deep networks typically requires large amounts of annotated data, which has limited their applications in fields where accessing annotations is difficult. Moreover, manual annotations are biased towards a specific task and towards the annotator&amp;rsquo;s own biases.</description>
    </item>
    
    <item>
      <title>Kai Han: Transformer in Transformer</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-2/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-2/</guid>
      <description>Title: Transformer in Transformer
Speaker: Kai Han, Huawei Noah’s Ark Lab
Date and Time: Tuesday, April 27, 1-2 pm
Place: Zoom Meeting
Meeting ID: 621 2899 7306 Pass code: 373072
Abstract: Transformer is a type of self-attention-based neural networks originally applied for NLP tasks. Recently, pure transformer-based models are proposed to solve computer vision problems. These visual transformers usually view an image as a sequence of patches while they ignore the intrinsic structure information inside each patch.</description>
    </item>
    
    <item>
      <title>Alaa El-Nouby: Training Vision Transformers for Image Retrieval</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-4/</link>
      <pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-4/</guid>
      <description>Title: Training Vision Transformers for Image Retrieval
Speaker: Alaa El-Nouby, Facebook AI Research and Inria Paris
Date and Time: Tuesday, April 20, 1-2 pm
Place: Zoom Meeting
Meeting ID: 699 6421 6598 Pass code: 600145
Abstract: Transformers have shown outstanding results for natural language understanding and, more recently, for image classification. We here extend this work and propose a transformer-based approach for image retrieval: we adopt vision transformers for generating image descriptors and train the resulting model with a metric learning objective, which combines a contrastive loss with a differential entropy regularizer.</description>
    </item>
    
    <item>
      <title>Hugo Touvron: Training data-efficient image transformers &amp; distillation through attention / Going deeper with Image Transformers</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-3/</link>
      <pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-3/</guid>
      <description>Title: Training data-efficient image transformers &amp;amp; distillation through attention / Going deeper with Image Transformers
Speaker: Hugo Touvron, Facebook AI Research and Sorbonne University
Date and Time: Tuesday, April 20, 1-2 pm
Place: Zoom Meeting
Meeting ID: 699 6421 6598 Pass code: 600145
Abstract:
Training data-efficient image transformers &amp;amp; distillation through attention
Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.</description>
    </item>
    
    <item>
      <title>Mostafa Dehghani: Scaling up Vision Models with Transformers</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-1/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-1/</guid>
      <description>Title: Scaling up Vision Models with Transformers
Speaker: Mostafa Dehghani, Google Brain
Date and Time: Tuesday, April 13, 1-2 pm
Place: Zoom Meeting
Meeting ID: 698 1002 6609 Pass code: 983918
Abstract: While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/about/</guid>
      <description>The KTH Machine Learning Seminars is a series of international presentations from the field of machine learning and computer vision on certain thematic topics of interest for the KTH Division of Robotics, Perception, and Learning. The seminars is announced in a mailing list that has subscribers from other universities and industry outside KTH. If you or your colleagues want to present at this series, or have similar questions, contact Hossein Azizpour at azizpour@kth.</description>
    </item>
    
    <item>
      <title>Alyosha Efros: The Revolution Will Not Be Supervised</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-6/</guid>
      <description>Title: The Revolution Will Not Be Supervised
Speaker: Alyosha Efros, University of California Berkeley
Date and Time: 2019
Place: Room 304, Teknikringen 14
Abstract: Computer vision has made impressive gains through the use of deep learning models trained with large-scale labeled data. However, labels require expertise and curation and are expensive to collect. Worse, semantic supervision often leads to models that can &amp;ldquo;cheat&amp;rdquo;. Can one discover useful visual representations without the use of explicitly curated labels?</description>
    </item>
    
    <item>
      <title>Daphna Weinshall</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-7/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/posts/post-7/</guid>
      <description>Title:
Speaker: Daphna Weinshall, Hebrew University, Jerusalem
Date and Time: 2019
Place: Room 525, Teknikringen 14
Abstract:
Bio:
Organizer: Stefan Carlsson</description>
    </item>
    
    <item>
      <title>Registration</title>
      <link>https://www.csc.kth.se/cvap/cvg/ml-seminars/registration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.csc.kth.se/cvap/cvg/ml-seminars/registration/</guid>
      <description>Please wait a moment for the registration form to load!
  window.addEventListener(&#39;load&#39;, function () { var options = {}; options.tags = []; options.host = &#39;https://www.kth.se&#39;; getEmbeddedForm(&#34;607986bfe946a10014f79392&#34;, options); }, { once: true });  </description>
    </item>
    
  </channel>
</rss>
